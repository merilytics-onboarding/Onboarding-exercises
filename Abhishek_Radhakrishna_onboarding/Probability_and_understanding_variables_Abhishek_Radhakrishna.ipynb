{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Probablity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Bayes theorem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bayes’ theorem is a way to figure out conditional probability. Conditional probability is the probability of an event \n",
    "#happening,given that it has some relationship to one or more other events. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Understanding Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Types of Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random variables: are associated with random processes and give numbers to outcomes of random events\n",
    "\n",
    "#Discrete variable: a variable that can only take on a certain number of values. For example,\n",
    "#“number of cars in a parking lot” is discrete because a car park can only hold so many cars.\n",
    "\n",
    "#Continuous variable: a variable with infinite number of values, like “time” or “weight”.\n",
    "    \n",
    "b#Categorical variable: variables than can be put into categories. For example, the category\n",
    "#“Toothpaste Brands” might contain the variables Colgate and Aquafresh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Confidence intervals and significance testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#In statistics, a confidence interval (CI) is a type of interval estimate, computed from the\n",
    "#statistics of the observed data, that might contain the true value of an unknown population parameter.\n",
    "#The interval has an associated confidence level that, loosely speaking, quantifies the level\n",
    "#of confidence that the parameter lies in the interval.\n",
    "\n",
    "\n",
    "#You can use confidence intervals (CIs) as an alternative to some of the usual significance tests. \n",
    "#To assess significance using CIs, you first define a number that measures the amount of effect you’re\n",
    "#testing for. This effect size can be the difference between two means or two proportions, the ratio of \n",
    "#two means, an odds ratio, a relative risk ratio, or a hazard ratio, among others.\n",
    "\n",
    "#The complete absence of any effect corresponds to a difference of 0, or a ratio of 1, so these are called \n",
    "#the “no-effect” values.\n",
    "#The following are always true:\n",
    "#-If the 95 percent CI around the observed effect size includes the no-effect value (0 for differences, 1 for ratios), \n",
    "#then the effect is not statistically significant (that is, a significance test for that effect will produce p > 0.05).\n",
    "#-If the 95 percent CI around the observed effect size does not include the no-effect value, then the effect is significant \n",
    "#(that is, a significance test for that effect will produce p </= 0.05)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 Comparing Distrubutions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.3.1 T-Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The t test tells you how significant the differences between groups are; In other words it lets you know if those \n",
    "#differences (measured in means/averages) could have happened by chance. \n",
    "\n",
    "#The t score is a ratio between the difference between two groups and the difference within the groups. The larger \n",
    "#the t score, the more difference there is between groups. \n",
    "\n",
    "#When you run a t test, the bigger the t-value, the more likely it is that the results are repeatable.\n",
    "\n",
    "#A large t-score tells you that the groups are different. \n",
    "#A small t-score tells you that the groups are similar. \n",
    "\n",
    "#Every t-value has a p-value to go with it. A p-value is the probability that the results from your sample data occurred \n",
    "#by chance. P-values are from 0% to 100%. They are usually written as a decimal. For example, a p value of 5% is 0.05. \n",
    "#Low p-values are good"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.3.2 Kolmogorov-smirnov test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The Kolmogorov-Smirnov Goodness of Fit Test (K-S test) compares your data with a known distribution and lets you know if \n",
    "#they have the same distribution. \n",
    "\n",
    "#The general steps to run the test are:\n",
    "#Create an EDF for your sample data (see Empirical Distribution Function for steps),\n",
    "#Specify a parent distribution (i.e. one that you want to compare your EDF to),\n",
    "#Graph the two distributions together.\n",
    "#Measure the greatest vertical distance between the two graphs.\n",
    "#Calculate the test statistic.\n",
    "#Find the critical value in the KS table.\n",
    "#Compare to the critical value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4 ANNOVA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Statistical inference procedures that concern the comparison of two populations cannot usually be applied to three \n",
    "#or more populations. To study more than two populations at once, we need different types of statistical tools. \n",
    "#Analysis of variance, or ANOVA, is a technique from statistical interference that allows us to deal with several populations.\n",
    "\n",
    "\n",
    "#Find the mean for each of the groups. \n",
    "#Find the overall mean (the mean of the groups combined). \n",
    "#Find the Within Group Variation; the total deviation of each member’s score from the Group Mean. \n",
    "#Find the Between Group Variation: the deviation of each Group Mean from the Overall Mean.\n",
    "#Find the F statistic: the ratio of Between Group Variation to Within Group Variation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.5 Chi sqaure test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A chi-square test for independence compares two variables in a contingency table to see if they are related. \n",
    "#In a more general sense, it tests to see whether distributions of categorical variables differ from each another. \n",
    "#A very small chi square test statistic means that your observed data fits your expected data extremely well. In other words, \n",
    "#there is a relationship. \n",
    "#A very large chi square test statistic means that the data does not fit very well. In other words, there isn’t a relationship."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.6 Correlation and Covariance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.6.1 Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Correlation – \n",
    "#It show whether and how strongly pairs of variables are related to each other. \n",
    "#Correlation takes values between -1 to +1, wherein values close to +1 represents strong positive correlation and values \n",
    "#close to -1 represents strong negative correlation. \n",
    "#In this variable are indirectly related to each other. \n",
    "#It gives the direction and strength of relationship between variables. \n",
    "\n",
    "#Pearson Correlation\n",
    "#The Pearson correlation evaluates the linear relationship between two continuous variables. \n",
    "#A relationship is linear when a change in one variable is associated with a proportional change in the other variable.\n",
    "\n",
    "#Spearman Correlation\n",
    "#The Spearman correlation evaluates the monotonic relationship between two continuous or ordinal variables. \n",
    "#In a monotonic relationship, the variables tend to change together, but not necessarily at a constant rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.6.2 Covariance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Covariance –\n",
    "#It is the relationship between a pair of random variables where change in one variable causes change in another variable. \n",
    "#It can take any value between -infinity to +infinity, where the negative value represents the negative relationship whereas a \n",
    "#positive value represents the positive relationship. \n",
    "#It is used for the linear relationship between variables. \n",
    "#It gives the direction of relationship between variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
